{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI's Language Models\n",
    "\n",
    "This notebook provides an explanatory guide to the first component of Retrieval-Augmented Generation (RAG), focusing on leveraging OpenAI's Large Language Models (LLMs). The key areas covered include:\n",
    "\n",
    "1. **Calling OpenAI**: Demonstrating a simple test message to understand how to interact with OpenAI's API.\n",
    "2. **Chat Completion Models**: Focusing on chat completion models in \"normal mode.\" While JSON mode can be useful in some scenarios, it is not the primary focus here.\n",
    "3. **Parameter Explanation**: Detailing important parameters and making a case for setting the temperature parameter to 0 for predictable outputs.\n",
    "4. **OpenAI Pricing**: Discussing the pricing model, including:\n",
    "    - **Tokens**: How usage is calculated based on tokens.\n",
    "    - **Different Models**: Exploring the various available models.\n",
    "    - **Website Updates**: Noting some discrepancies on OpenAI's website, such as references to models that no longer exist.\n",
    "\n",
    "This introduction will set the stage for understanding the integration of OpenAI's models within the RAG framework, ensuring reliable and accurate outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the OpenAI API key from the .env file into the environment variable called OPENAI_API_KEY\n",
    "load_dotenv()\n",
    "\n",
    "# Instantiate the OpenAI client\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling the OpenAI api\n",
    "\n",
    "### The call:\n",
    "```python\n",
    "response = client.chat.completions.create()\n",
    "```\n",
    "- client: The object representing your OpenAI client, which interfaces with the API.\n",
    "- chat: The namespace for handling chat-based requests.\n",
    "- completions: A sub-namespace for generating completions in a chat model.\n",
    "- create(): The method that sends a request to the API to generate a response.\n",
    "\n",
    "### Method attributes:\n",
    "- model: Specifies which model to use (e.g., GPT-4).\n",
    "- messages: A list of messages representing the conversation or inputs to the model.\n",
    "- max_tokens: Limits the length of the modelâ€™s output (in tokens).\n",
    "- temperature: Controls the randomness or creativity of the output.\n",
    "- n: The number of completions (responses) the model will generate.\n",
    "\n",
    "### Why ```choices[0]``` is Always Indexed at 0? \n",
    "The choices array in the API response contains all the generated completions. By default, the array has just one completion (so it's accessed via choices[0]). If you specify n>1, the API returns multiple completions, and you can loop through choices to access each completion individually.\n",
    "\n",
    "### Most important setting\n",
    "- Temperature == 1 maximizes creativity\n",
    "- Temperature == 0 ensures reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors for the chat messages\n",
    "PASTELS = [('Pale Red', '#ffcccc'), ('Pale Green', '#ccffcc'), ('Pale Blue', '#cceeff')]\n",
    "\n",
    "# We need a message for both the user and the system to start the conversation, the roles are 'user' and 'system'\n",
    "TEST_MESSAGE = [\n",
    "        { \"role\": \"system\",\n",
    "          \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "           \"role\": \"user\",\n",
    "           \"content\": \"If Sarah is older than Tom, and Tom is older than Jane, who is the youngest and how do you know?\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div style='color: #ffcccc;'>If Sarah is older than Tom, and Tom is older than Jane, then Jane is the youngest. This is because the relationships indicate that Sarah > Tom > Jane in terms of age, meaning Jane is at the end of this sequence and therefore the youngest.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style='color: #ccffcc;'>If Sarah is older than Tom, and Tom is older than Jane, we can deduce the following age order:\n",
       "\n",
       "1. Sarah is older than Tom.\n",
       "2. Tom is older than Jane.\n",
       "\n",
       "From this information, we can conclude that Jane is the youngest, because she is younger than both Tom and Sarah. Therefore, the youngest is Jane.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style='color: #cceeff;'>If Sarah is older than Tom, and Tom is older than Jane, then Jane is the youngest. We know this because the statements provide a clear order of ages: Sarah > Tom > Jane, indicating that Jane is at the end of this sequence and thus the youngest.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Temperature == 1 maximizes creativity, but also randomness\n",
    "completion = client.chat.completions.create(\n",
    "      model=\"gpt-4o-mini\"\n",
    "    , messages=TEST_MESSAGE\n",
    "    , max_tokens=100 # limits the outputted completion_tokens, so will be cut short if it exceeds max_tokens tokens\n",
    "    , temperature=1  # the randomness of the output, 0 is deterministic, 1 is random (repated calls will give different results for 1)\n",
    "    , n=3            # This will generate 3 separate completions\n",
    ")\n",
    "\n",
    "for i, choice in enumerate(completion.choices):\n",
    "    color = PASTELS[i][1]\n",
    "    display(Markdown(f\"<div style='color: {color};'>{choice.message.content}</div>\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The exact same call, but with temperature == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div style='color: #ffcccc;'>If Sarah is older than Tom, and Tom is older than Jane, then Jane is the youngest. \n",
       "\n",
       "We can determine this by analyzing the relationships:\n",
       "- Sarah > Tom (Sarah is older than Tom)\n",
       "- Tom > Jane (Tom is older than Jane)\n",
       "\n",
       "From these two statements, we can infer that:\n",
       "- Sarah > Tom > Jane\n",
       "\n",
       "Since Jane is at the end of this chain, she is the youngest among the three.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style='color: #ccffcc;'>If Sarah is older than Tom, and Tom is older than Jane, then Jane is the youngest. \n",
       "\n",
       "We can determine this by analyzing the relationships:\n",
       "- Sarah > Tom (Sarah is older than Tom)\n",
       "- Tom > Jane (Tom is older than Jane)\n",
       "\n",
       "From these two statements, we can infer that:\n",
       "- Sarah > Tom > Jane\n",
       "\n",
       "Since Jane is at the end of this chain, she is the youngest among the three.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style='color: #cceeff;'>If Sarah is older than Tom, and Tom is older than Jane, then Jane is the youngest. \n",
       "\n",
       "We can determine this by analyzing the relationships:\n",
       "- Sarah > Tom (Sarah is older than Tom)\n",
       "- Tom > Jane (Tom is older than Jane)\n",
       "\n",
       "From these two statements, we can infer that:\n",
       "- Sarah > Tom > Jane\n",
       "\n",
       "Since Jane is at the end of this chain, she is the youngest among the three.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Temperature == 1 maximizes creativity, but also randomness\n",
    "completion = client.chat.completions.create(\n",
    "      model=\"gpt-4o-mini\"\n",
    "    , messages=TEST_MESSAGE\n",
    "    , max_tokens=100 # limits the outputted completion_tokens, so will be cut short if it exceeds max_tokens tokens\n",
    "    , temperature=0  # the randomness of the output, 0 is deterministic, 1 is random (repated calls will give different results for 1)\n",
    "    , n=3            # This will generate 3 separate completions\n",
    ")\n",
    "\n",
    "for i, choice in enumerate(completion.choices):\n",
    "    color = PASTELS[i][1]\n",
    "    display(Markdown(f\"<div style='color: {color};'>{choice.message.content}</div>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `temperature = 0` in a system that needs to be tested for answer accuracy is justified because it ensures deterministic and consistent outputs from the model. Here's why:\n",
    "\n",
    "1. **Determinism and Reproducibility**: A temperature of 0 makes the model always choose the highest probability response for a given input. This eliminates randomness and guarantees that the same input will consistently produce the same output, which is crucial when testing the accuracy of the system. In scenarios where you need to compare results across multiple runs, having deterministic outputs allows for clear comparisons.\n",
    "\n",
    "2. **Focus on Accuracy**: When testing for accuracy, it is important to avoid introducing variability. Higher temperatures introduce randomness in the sampling process, leading to more creative but less predictable outputs. By setting temperature to 0, the system is more likely to generate the \"most likely\" or \"correct\" answer based on its training data, which is essential for evaluating accuracy.\n",
    "\n",
    "3. **Error Identification**: With a consistent response at `temperature = 0`, it is easier to identify errors or inaccuracies in the model's predictions. If the model consistently generates incorrect answers for certain inputs, you can more reliably track patterns or weaknesses that need to be addressed.\n",
    "\n",
    "Thus, in a testing scenario where accuracy is the primary goal, using `temperature = 0` is an effective way to ensure precision and consistency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI's Pricing\n",
    "OpenAI uses a [**pay-as-you-go**](https://openai.com/api/pricing/) pricing model, which means you only pay for what you use. The costs are based on the number of tokens processed.\n",
    " - [**Calculating LLM Token Counts: A Practical Guide**](https://winder.ai/calculating-token-counts-llm-context-windows-practical-guide/) - an article explaining what tokens are all about \n",
    " - [**Tokenizer**](https://platform.openai.com/tokenizer) - an online tool that can be used to tokenize our text input. The tool colors the different tokens to help us get a visual understanding of the details\n",
    "\n",
    "   \n",
    "Tell them that tokens will be explained in more detail in the next session, when we will be talking about text embeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PRICING_PER_M_TOKENS = {\n",
    "    'gpt-4o': {'prompt_tokens': 5.00, 'completion_tokens': 15.00},\n",
    "    'gpt-4o-2024-08-06': {'prompt_tokens': 2.50, 'completion_tokens': 10.00},\n",
    "    'gpt-4o-mini': {'prompt_tokens': 0.150, 'completion_tokens': 0.600},\n",
    "    'gpt-4o-mini-2024-07-18': {'prompt_tokens': 0.150, 'completion_tokens': 0.600},\n",
    "    'o1-preview': {'prompt_tokens': 15.00, 'completion_tokens': 60.00}\n",
    "}\n",
    "\n",
    "def model(persona, prompt, model=\"gpt-4o-mini\"):\n",
    "    completion = client.chat.completions.create(\n",
    "          model=model\n",
    "        , messages=[\n",
    "            { \"role\": \"system\", \"content\": persona},\n",
    "            { \"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "        , temperature=0\n",
    "    )\n",
    "    # Get the pricing for the model used in the completion\n",
    "    pricing = MODEL_PRICING_PER_M_TOKENS[completion.model]\n",
    "\n",
    "    # Calculate the cost of the completion\n",
    "    prompt_cost = completion.usage.prompt_tokens * pricing['prompt_tokens']\n",
    "    generation_cost = completion.usage.completion_tokens * pricing['completion_tokens']\n",
    "    total_cost = (prompt_cost + generation_cost) / 10**6\n",
    "\n",
    "    # Extract the message from the completion\n",
    "    message = completion.choices[0].message.content\n",
    "\n",
    "    return message, total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The capital of France is Paris.', 7.65e-06)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\"you are a helpful assistant\", \"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The capital of France is Paris.', 0.0001275)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\"you are a helpful assistant\", \"What is the capital of France?\", model='gpt-4o-2024-08-06')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
