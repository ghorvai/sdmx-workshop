{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI's Language Models\n",
    "\n",
    "This notebook provides an explanatory guide to the first component of Retrieval-Augmented Generation (RAG), focusing on leveraging OpenAI's Large Language Models (LLMs). The key areas covered include:\n",
    "\n",
    "1. **Calling OpenAI**: Demonstrating a simple test message to understand how to interact with OpenAI's API.\n",
    "2. **Chat Completion Models**: Focusing on chat completion models in \"normal mode.\" While JSON mode can be useful in some scenarios, it is not the primary focus here.\n",
    "3. **Parameter Explanation**: Detailing important parameters and making a case for setting the temperature parameter to 0 for predictable outputs.\n",
    "4. **OpenAI Pricing**: Discussing the pricing model, including:\n",
    "    - **Tokens**: How usage is calculated based on tokens.\n",
    "    - **Different Models**: Exploring the various available models.\n",
    "    - **Website Updates**: Noting some discrepancies on OpenAI's website, such as references to models that no longer exist.\n",
    "\n",
    "This introduction will set the stage for understanding the integration of OpenAI's models within the RAG framework, ensuring reliable and accurate outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the OpenAI API key from the .env file into the environment variable called OPENAI_API_KEY\n",
    "load_dotenv()\n",
    "\n",
    "# Instantiate the OpenAI client\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling the OpenAI api\n",
    "\n",
    "### The call:\n",
    "```python\n",
    "response = client.chat.completions.create()\n",
    "```\n",
    "- client: The object representing your OpenAI client, which interfaces with the API.\n",
    "- chat: The namespace for handling chat-based requests.\n",
    "- completions: A sub-namespace for generating completions in a chat model.\n",
    "- create(): The method that sends a request to the API to generate a response.\n",
    "\n",
    "### Method attributes:\n",
    "- model: Specifies which model to use (e.g., GPT-4).\n",
    "- messages: A list of messages representing the conversation or inputs to the model.\n",
    "- max_tokens: Limits the length of the modelâ€™s output (in tokens).\n",
    "- temperature: Controls the randomness or creativity of the output.\n",
    "- n: The number of completions (responses) the model will generate.\n",
    "\n",
    "### Why ```choices[0]``` is Always Indexed at 0? \n",
    "The choices array in the API response contains all the generated completions. By default, the array has just one completion (so it's accessed via choices[0]). If you specify n>1, the API returns multiple completions, and you can loop through choices to access each completion individually.\n",
    "\n",
    "### Most important setting\n",
    "- Temperature == 1 maximizes creativity\n",
    "- Temperature == 0 ensures reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors for the chat messages\n",
    "PASTELS = [('Pale Red', '#ffcccc'), ('Pale Green', '#ccffcc'), ('Pale Blue', '#cceeff')]\n",
    "\n",
    "# We need a message for both the user and the system to start the conversation, the roles are 'user' and 'system'\n",
    "TEST_MESSAGE = [\n",
    "        { \"role\": \"system\",\n",
    "          \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "           \"role\": \"user\",\n",
    "           \"content\": \"If Sarah is older than Tom, and Tom is older than Jane, who is the youngest and how do you know?\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div style='background-color: #ffcccc; color: black;'>Based on the information provided:\n",
       "\n",
       "1. Sarah is older than Tom.\n",
       "2. Tom is older than Jane.\n",
       "\n",
       "From this, we can conclude the following order of ages:\n",
       "\n",
       "- Sarah > Tom > Jane\n",
       "\n",
       "Since Jane is at the end of this order, it indicates that Jane is the youngest. Therefore, Jane is the youngest because she is younger than both Tom and Sarah.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style='background-color: #ccffcc; color: black;'>Based on the information provided, if Sarah is older than Tom, and Tom is older than Jane, we can establish the following order of age from oldest to youngest:\n",
       "\n",
       "1. Sarah (oldest)\n",
       "2. Tom\n",
       "3. Jane (youngest)\n",
       "\n",
       "From this order, we can conclude that Jane is the youngest because both Sarah and Tom are older than her.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style='background-color: #cceeff; color: black;'>If Sarah is older than Tom, and Tom is older than Jane, then Jane is the youngest. \n",
       "\n",
       "We can determine this from the relationships given:\n",
       "- Sarah > Tom (Sarah is older than Tom)\n",
       "- Tom > Jane (Tom is older than Jane)\n",
       "\n",
       "Since both Sarah and Tom are older than Jane, it logically follows that Jane must be the youngest among the three.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Temperature == 1 maximizes creativity, but also randomness\n",
    "completion = client.chat.completions.create(\n",
    "      model=\"gpt-4o-mini\"\n",
    "    , messages=TEST_MESSAGE\n",
    "    , max_tokens=100 # limits the outputted completion_tokens, so will be cut short if it exceeds max_tokens tokens\n",
    "    , temperature=1  # the randomness of the output, 0 is deterministic, 1 is random (repated calls will give different results for 1)\n",
    "    , n=3            # This will generate 3 separate completions\n",
    ")\n",
    "\n",
    "for i, choice in enumerate(completion.choices):\n",
    "    color = PASTELS[i][1]\n",
    "    display(Markdown(f\"<div style='background-color: {color}; color: black;'>{choice.message.content}</div>\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The exact same call, but with temperature == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div style='background-color: #ffcccc; color: black;'>If Sarah is older than Tom, and Tom is older than Jane, then Jane is the youngest. \n",
       "\n",
       "We can determine this by analyzing the relationships:\n",
       "- Sarah > Tom (Sarah is older than Tom)\n",
       "- Tom > Jane (Tom is older than Jane)\n",
       "\n",
       "From these two statements, we can infer that:\n",
       "- Sarah > Tom > Jane\n",
       "\n",
       "Since Jane is at the end of this chain, she is the youngest.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style='background-color: #ccffcc; color: black;'>If Sarah is older than Tom, and Tom is older than Jane, then Jane is the youngest. \n",
       "\n",
       "We can determine this by analyzing the relationships:\n",
       "- Sarah > Tom (Sarah is older than Tom)\n",
       "- Tom > Jane (Tom is older than Jane)\n",
       "\n",
       "From these two statements, we can infer that:\n",
       "- Sarah > Tom > Jane\n",
       "\n",
       "Since Jane is at the end of this chain, she is the youngest.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style='background-color: #cceeff; color: black;'>If Sarah is older than Tom, and Tom is older than Jane, then Jane is the youngest. \n",
       "\n",
       "We can determine this by analyzing the relationships:\n",
       "- Sarah > Tom (Sarah is older than Tom)\n",
       "- Tom > Jane (Tom is older than Jane)\n",
       "\n",
       "From these two statements, we can infer that:\n",
       "- Sarah > Tom > Jane\n",
       "\n",
       "Since Jane is at the end of this chain, she is the youngest.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Temperature == 1 maximizes creativity, but also randomness\n",
    "completion = client.chat.completions.create(\n",
    "      model=\"gpt-4o-mini\"\n",
    "    , messages=TEST_MESSAGE\n",
    "    , max_tokens=100 # limits the outputted completion_tokens, so will be cut short if it exceeds max_tokens tokens\n",
    "    , temperature=0  # the randomness of the output, 0 is deterministic, 1 is random (repated calls will give different results for 1)\n",
    "    , n=3            # This will generate 3 separate completions\n",
    ")\n",
    "\n",
    "for i, choice in enumerate(completion.choices):\n",
    "    color = PASTELS[i][1]\n",
    "    display(Markdown(f\"<div style='background-color: {color}; color: black;'>{choice.message.content}</div>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `temperature = 0` in a system that needs to be tested for answer accuracy is justified because it ensures deterministic and consistent outputs from the model. Here's why:\n",
    "\n",
    "1. **Determinism and Reproducibility**: A temperature of 0 makes the model always choose the highest probability response for a given input. This eliminates randomness and guarantees that the same input will consistently produce the same output, which is crucial when testing the accuracy of the system. In scenarios where you need to compare results across multiple runs, having deterministic outputs allows for clear comparisons.\n",
    "\n",
    "2. **Focus on Accuracy**: When testing for accuracy, it is important to avoid introducing variability. Higher temperatures introduce randomness in the sampling process, leading to more creative but less predictable outputs. By setting temperature to 0, the system is more likely to generate the \"most likely\" or \"correct\" answer based on its training data, which is essential for evaluating accuracy.\n",
    "\n",
    "3. **Error Identification**: With a consistent response at `temperature = 0`, it is easier to identify errors or inaccuracies in the model's predictions. If the model consistently generates incorrect answers for certain inputs, you can more reliably track patterns or weaknesses that need to be addressed.\n",
    "\n",
    "Thus, in a testing scenario where accuracy is the primary goal, using `temperature = 0` is an effective way to ensure precision and consistency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI's Pricing\n",
    "OpenAI uses a [**pay-as-you-go**](https://openai.com/api/pricing/) pricing model, which means you only pay for what you use. The costs are based on the number of tokens processed.\n",
    " - [**Calculating LLM Token Counts: A Practical Guide**](https://winder.ai/calculating-token-counts-llm-context-windows-practical-guide/) - an article explaining what tokens are all about \n",
    " - [**Tokenizer**](https://platform.openai.com/tokenizer) - an online tool that can be used to tokenize our text input. The tool colors the different tokens to help us get a visual understanding of the details\n",
    "\n",
    "   \n",
    "Tell them that tokens will be explained in more detail in the next session, when we will be talking about text embeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not all model names are working despite copying the information directly from the Openai website... ðŸ˜³\n",
    "MODEL_PRICING_PER_M_TOKENS = {\n",
    "    'gpt-4o': {'prompt_tokens': 5.00, 'completion_tokens': 15.00},\n",
    "    'gpt-4o-2024-08-06': {'prompt_tokens': 2.50, 'completion_tokens': 10.00},\n",
    "    'gpt-4o-mini': {'prompt_tokens': 0.150, 'completion_tokens': 0.600},\n",
    "    'gpt-4o-mini-2024-07-18': {'prompt_tokens': 0.150, 'completion_tokens': 0.600},\n",
    "    'o1-preview': {'prompt_tokens': 15.00, 'completion_tokens': 60.00}\n",
    "}\n",
    "\n",
    "def model(persona, prompt, model=\"gpt-4o-mini\"):\n",
    "    completion = client.chat.completions.create(\n",
    "          model=model\n",
    "        , messages=[\n",
    "            { \"role\": \"system\", \"content\": persona},\n",
    "            { \"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "        , temperature=0\n",
    "    )\n",
    "    # Get the pricing for the model used in the completion\n",
    "    pricing = MODEL_PRICING_PER_M_TOKENS[completion.model]\n",
    "\n",
    "    # Calculate the cost of the completion\n",
    "    prompt_cost = completion.usage.prompt_tokens * pricing['prompt_tokens']\n",
    "    generation_cost = completion.usage.completion_tokens * pricing['completion_tokens']\n",
    "    total_cost = (prompt_cost + generation_cost) / 10**6\n",
    "\n",
    "    # Extract the message from the completion\n",
    "    message = completion.choices[0].message.content\n",
    "\n",
    "    return message, total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The capital of France is Paris.', 7.65e-06)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\"you are a helpful assistant\", \"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The capital of France is Paris.', 0.0001275)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can get the same answer for more money by insisting on using the gpt-4o model instead of the gpt-4o-mini ðŸ˜€:\n",
    "model(\"you are a helpful assistant\", \"What is the capital of France?\", model='gpt-4o-2024-08-06')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging and Pricing: Key Features of the OpenAI Integration\n",
    "\n",
    "This code includes two important features that help manage the use of the OpenAI API: logging and pricing calculations.\n",
    "\n",
    "### Logging Interactions\n",
    "\n",
    "The code defines a decorator function called `log_interaction` that wraps the main `model` function. This decorator is responsible for:\n",
    "\n",
    "1. Logging the user's prompt to a file called `llm_interactions.log`.\n",
    "2. Calling the `model` function to get the AI's response and the cost of the interaction.\n",
    "3. Logging the AI's response and the cost of the interaction to the same log file.\n",
    "4. Updating a global variable called `total_price` to keep track of the total cost of all interactions.\n",
    "\n",
    "This logging functionality is important for tracking the usage of the OpenAI API, both in terms of the content of the interactions and the associated costs.\n",
    "\n",
    "### Calculating Pricing\n",
    "\n",
    "The code also includes a dictionary called `MODEL_PRICING_PER_M_TOKENS` that stores the pricing information for different OpenAI models. This pricing information is used in the `model` function to calculate the cost of each interaction.\n",
    "\n",
    "The `model` function first gets the pricing information for the specific model used in the completion. It then calculates the cost of the interaction by:\n",
    "\n",
    "1. Multiplying the number of prompt tokens by the prompt token price.\n",
    "2. Multiplying the number of completion tokens by the completion token price.\n",
    "3. Adding the prompt cost and generation cost, and dividing the total by 1 million to get the cost in dollars.\n",
    "\n",
    "This accurate pricing calculation is important for understanding the financial implications of using the OpenAI API, especially for applications that may generate a large number of requests.\n",
    "\n",
    "By combining these logging and pricing features, the code provides a robust and transparent way to manage the use of the OpenAI API, making it easier to track usage, costs, and potential issues that may arise during the integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focused Explanation: Logging and Price Calculation\n",
    "\n",
    "This code includes two important features: a **logging decorator** to keep track of user interactions and a **price calculation** system to manage OpenAI API costs.\n",
    "\n",
    "---\n",
    "\n",
    "### Logging Decorator (`log_interaction`)\n",
    "\n",
    "The `log_interaction` decorator is a function that \"wraps\" around the main `model` function to add extra functionalityâ€”specifically logging and cost trackingâ€”each time the `model` function is called.\n",
    "\n",
    "- **Purpose**: The decorator records each user input, the AIâ€™s response, and the cost of each interaction in a log file (`llm_interactions.log`).\n",
    "- **How it Works**:\n",
    "  - Before calling `model`, it logs the userâ€™s message.\n",
    "  - After `model` returns, it logs the AI's response and the cost of the interaction.\n",
    "  - Finally, it updates a running total of all interaction costs and logs that cumulative total as well.\n",
    "\n",
    "This way, each prompt and response pair, along with its cost, is recorded for easy tracking.\n",
    "\n",
    "### Price Calculation in `model`\n",
    "\n",
    "The `model` function calculates the cost of each API call based on **token usage**. OpenAI charges per token, and the cost varies by model. Hereâ€™s how it works:\n",
    "\n",
    "1. **Retrieve Token Counts**: \n",
    "   - When the API responds, it includes `prompt_tokens` (tokens used in the prompt) and `completion_tokens` (tokens generated in the response).\n",
    "   \n",
    "2. **Look Up Model Pricing**: \n",
    "   - Each model has different rates, stored in the `MODEL_PRICING_PER_M_TOKENS` dictionary. For example, `gpt-4o-mini` might charge $0.150 per 1,000 prompt tokens and $0.600 per 1,000 completion tokens.\n",
    "\n",
    "3. **Calculate Total Cost**:\n",
    "   - The code calculates the cost of the prompt and the response separately:\n",
    "     - `prompt_cost = prompt_tokens * rate_per_prompt_token`\n",
    "     - `generation_cost = completion_tokens * rate_per_completion_token`\n",
    "   - It then combines these two to get the total cost of the interaction, dividing by 1,000,000 to convert from tokens to dollars.\n",
    "\n",
    "4. **Return Cost**:\n",
    "   - The total cost for that interaction is returned alongside the AIâ€™s response, which the decorator then logs.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "Together, the logging decorator and price calculation allow the code to:\n",
    "- **Track each interaction** with the AI, logging both input and output.\n",
    "- **Calculate and log the cost** of each call, giving a transparent view of API usage expenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "#from datetime import datetime\n",
    "from functools import wraps\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='llm_interactions.log', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Global variable to keep track of total price\n",
    "total_price = 0\n",
    "\n",
    "\n",
    "MODEL_PRICING_PER_M_TOKENS = {\n",
    "    'gpt-4o': {'prompt_tokens': 5.00, 'completion_tokens': 15.00},\n",
    "    'gpt-4o-2024-08-06': {'prompt_tokens': 2.50, 'completion_tokens': 10.00},\n",
    "    'gpt-4o-mini': {'prompt_tokens': 0.150, 'completion_tokens': 0.600},\n",
    "    'gpt-4o-mini-2024-07-18': {'prompt_tokens': 0.150, 'completion_tokens': 0.600},\n",
    "    'o1-preview': {'prompt_tokens': 15.00, 'completion_tokens': 60.00}\n",
    "}\n",
    "\n",
    "def log_interaction(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(persona, prompt, model=\"gpt-4o-mini\"):\n",
    "        global total_price\n",
    "        \n",
    "        # Log user message\n",
    "        logging.info(f\"User: {prompt}\")\n",
    "        \n",
    "        # Call the original function\n",
    "        message, cost = func(persona, prompt, model)\n",
    "        \n",
    "        # Log LLM response and cost\n",
    "        logging.info(f\"LLM: {message}\")\n",
    "        logging.info(f\"Cost of this interaction: ${cost:.6f}\")\n",
    "        \n",
    "        # Update total price\n",
    "        total_price += cost\n",
    "        logging.info(f\"Total price so far: ${total_price:.6f}\")\n",
    "        \n",
    "        return message, cost\n",
    "    return wrapper\n",
    "\n",
    "@log_interaction\n",
    "def model(persona, prompt, model=\"gpt-4o-mini\"):\n",
    "    completion = client.chat.completions.create(\n",
    "          model=model\n",
    "        , messages=[\n",
    "            { \"role\": \"system\", \"content\": persona},\n",
    "            { \"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "        , temperature=0\n",
    "    )\n",
    "    # Get the pricing for the model used in the completion\n",
    "    pricing = MODEL_PRICING_PER_M_TOKENS[completion.model]\n",
    "\n",
    "    # Calculate the cost of the completion\n",
    "    prompt_cost = completion.usage.prompt_tokens * pricing['prompt_tokens']\n",
    "    generation_cost = completion.usage.completion_tokens * pricing['completion_tokens']\n",
    "    total_cost = (prompt_cost + generation_cost) / 10**6\n",
    "\n",
    "    # Extract the message from the completion\n",
    "    message = completion.choices[0].message.content\n",
    "\n",
    "    return message, total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please observe the creation of ```llm_interactions.log``` file after the execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The capital of France is Paris.\n",
      "Cost: $0.000008\n",
      "Response: The capital of England is London.\n",
      "Cost: $0.000008\n",
      "Total cost of all interactions: $0.000016\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # first interaction\n",
    "    persona = \"You are a helpful assistant.\"\n",
    "    prompt = \"What is the capital of France?\"\n",
    "    response, cost = model(persona, prompt)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(f\"Cost: ${cost:.6f}\")\n",
    "\n",
    "    # second interaction\n",
    "    prompt = \"What is the capital of England?\"\n",
    "    response, cost = model(persona, prompt)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(f\"Cost: ${cost:.6f}\")\n",
    "\n",
    "    # print cumulative cost\n",
    "    print(f\"Total cost of all interactions: ${total_price:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
