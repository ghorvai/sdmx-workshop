{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ask ChatGPT an SDMX question => observ the failure (the same we used in the kick-off with Daniel)\n",
    "2. Explain the need for RAG [contextual_RAG](https://www.anthropic.com/news/contextual-retrieval) <- we only need a symple rag schema\n",
    "3. Explain how to use openai from python\n",
    "    1. explain the openai client and how to use it\n",
    "    2. tokens\n",
    "4. Textembedding\n",
    "    - what is it & why is it needed for RAG (context size reduction)\n",
    "        - talk a little bit more about tokens & vectors (king - man + woman = queen)\n",
    "    - Semantic search example\n",
    "5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/13/a1/601773020ad98d6d55f45c4482ea0c30af1351827eaab98cbc7d27b4716d/openai-1.47.1-py3-none-any.whl.metadata\n",
      "  Downloading openai-1.47.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Obtaining dependency information for anyio<5,>=3.5.0 from https://files.pythonhosted.org/packages/9e/ef/7a4f225581a0d7886ea28359179cb861d7fbcdefad29663fc1167b86f69f/anyio-4.6.0-py3-none-any.whl.metadata\n",
      "  Downloading anyio-4.6.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Obtaining dependency information for distro<2,>=1.7.0 from https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl.metadata\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Obtaining dependency information for httpx<1,>=0.23.0 from https://files.pythonhosted.org/packages/56/95/9377bcb415797e44274b51d46e3249eba641711cf3348050f76ee7b15ffc/httpx-0.27.2-py3-none-any.whl.metadata\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Obtaining dependency information for jiter<1,>=0.4.0 from https://files.pythonhosted.org/packages/03/eb/2308fa5f5c14c97c4c7720fef9465f1fa0771826cddb4eec9866bdd88846/jiter-0.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading jiter-0.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.6 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Obtaining dependency information for pydantic<3,>=1.9.0 from https://files.pythonhosted.org/packages/df/e4/ba44652d562cbf0bf320e0f3810206149c8a4e99cdbf66da82e97ab53a15/pydantic-2.9.2-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sniffio (from openai)\n",
      "  Obtaining dependency information for sniffio from https://files.pythonhosted.org/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl.metadata\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Obtaining dependency information for tqdm>4 from https://files.pythonhosted.org/packages/48/5d/acf5905c36149bbaec41ccf7f2b68814647347b72075ac0b1fe3022fdc73/tqdm-4.66.5-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions<5,>=4.11 in ./.venv/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai)\n",
      "  Obtaining dependency information for idna>=2.8 from https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl.metadata\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai)\n",
      "  Obtaining dependency information for certifi from https://files.pythonhosted.org/packages/12/90/3c9ff0512038035f59d279fddeb79f5f1eccd8859f06d6163c58798b9487/certifi-2024.8.30-py3-none-any.whl.metadata\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/78/d4/e5d7e4f2174f8a4d63c8897d79eb8fe2503f7ecc03282fee1fa2719c2704/httpcore-1.0.5-py3-none-any.whl.metadata\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Obtaining dependency information for h11<0.15,>=0.13 from https://files.pythonhosted.org/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl.metadata\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Obtaining dependency information for annotated-types>=0.6.0 from https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl.metadata\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<3,>=1.9.0->openai)\n",
      "  Obtaining dependency information for pydantic-core==2.23.4 from https://files.pythonhosted.org/packages/1d/9a/b634442e1253bc6889c87afe8bb59447f106ee042140bd57680b3b113ec7/pydantic_core-2.23.4-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading pydantic_core-2.23.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Downloading openai-1.47.1-py3-none-any.whl (375 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.6/375.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.6.0-py3-none-any.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.6/89.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Downloading jiter-0.5.0-cp311-cp311-macosx_11_0_arm64.whl (299 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.3/299.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.23.4-cp311-cp311-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.3/167.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: tqdm, sniffio, pydantic-core, jiter, idna, h11, distro, certifi, annotated-types, pydantic, httpcore, anyio, httpx, openai\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.6.0 certifi-2024.8.30 distro-1.9.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 idna-3.10 jiter-0.5.0 openai-1.47.1 pydantic-2.9.2 pydantic-core-2.23.4 sniffio-1.3.1 tqdm-4.66.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load the OpenAI API key from a file into an environment variable called OPENAI_API_KEY\n",
    "%run .load_openai_api_key.py\n",
    "\n",
    "# Instantiate the OpenAI client\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI's Pricing\n",
    "OpenAI uses a [**pay-as-you-go**](https://openai.com/api/pricing/) pricing model, which means you only pay for what you use. The costs are based on the number of tokens processed.\n",
    " - [**Tokens**](https://winder.ai/calculating-token-counts-llm-context-windows-practical-guide/) <- read this, maybe generate tokenized text with colors...\n",
    " - [Tokenizer](https://platform.openai.com/tokenizer) <- just a reference\n",
    " - [batch_mode](https://platform.openai.com/docs/guides/batch/overview) is cheapest, but using it requires advanced controlflow or offline tasks. \n",
    "   \n",
    "Tell them that tokens will be explained in more detail in the next session, when we will be talking about text embeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://openai.com/api/pricing/\n",
    "MODEL_PRICING_PER_M_TOKENS = {\n",
    "    'gpt-4o': {'prompt_tokens': 5.00, 'completion_tokens': 15.00},\n",
    "    'gpt-4o-2024-08-06': {'prompt_tokens': 2.50, 'completion_tokens': 10.00},\n",
    "    'gpt-4o-mini': {'prompt_tokens': 0.150, 'completion_tokens': 0.600},\n",
    "    'gpt-4o-mini-2024-07-18': {'prompt_tokens': 0.150, 'completion_tokens': 0.600},\n",
    "    'o1-preview': {'prompt_tokens': 15.00, 'completion_tokens': 60.00}\n",
    "}\n",
    "\n",
    "PASTELS = [\n",
    "      ('Pale Red', '#ffcccc')\n",
    "    , ('Pale Green', '#ccffcc')\n",
    "    , ('Pale Blue', '#cceeff')\n",
    "    , ('Pale Yellow', '#ffffcc')\n",
    "    , ('Pale Pink', '#ffccff')\n",
    "    , ('Pale Orange', '#ffebcc')\n",
    "    , ('Pale Purple', '#e6ccff')\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling the OpenAI api\n",
    "\n",
    "### The call:\n",
    "```python\n",
    "response = client.chat.completions.create()\n",
    "```\n",
    "- client: The object representing your OpenAI client, which interfaces with the API.\n",
    "- chat: The namespace for handling chat-based requests.\n",
    "- completions: A sub-namespace for generating completions in a chat model.\n",
    "- create(): The method that sends a request to the API to generate a response.\n",
    "\n",
    "### Method attributes:\n",
    "- model: Specifies which model to use (e.g., GPT-4).\n",
    "- messages: A list of messages representing the conversation or inputs to the model.\n",
    "- max_tokens: Limits the length of the model’s output (in tokens).\n",
    "- temperature: Controls the randomness or creativity of the output.\n",
    "- n: The number of completions (responses) the model will generate.\n",
    "\n",
    "### \n",
    "Why choices[0] is Always Indexed at 0:  \n",
    "The choices array in the API response contains all the generated completions. By default, the array has just one completion (so it's accessed via choices[0]). If you specify n>1, the API returns multiple completions, and you can loop through choices to access each completion individually.\n",
    "\n",
    "### Most important setting\n",
    "- Temperature == 1 maximizes creativity\n",
    "- Temperature == 0 ensures reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_message = [\n",
    "        { \"role\": \"system\",\n",
    "          \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "           \"role\": \"user\",\n",
    "           \"content\": \"If Sarah is older than Tom, and Tom is older than Jane, who is the youngest and how do you know?\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div style='color: #ffcccc;'>Based on the information provided: \n",
       "\n",
       "1. Sarah is older than Tom.\n",
       "2. Tom is older than Jane.\n",
       "\n",
       "From these two statements, we can deduce the following order of age (from oldest to youngest): Sarah > Tom > Jane.\n",
       "\n",
       "Since Jane is the last in this order, she is the youngest. Therefore, Jane is the youngest person among the three.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style='color: #ccffcc;'>If Sarah is older than Tom, and Tom is older than Jane, then we can conclude the following:\n",
       "\n",
       "- Sarah > Tom (Sarah is older than Tom)\n",
       "- Tom > Jane (Tom is older than Jane)\n",
       "\n",
       "From these two statements, we can infer that Sarah is the oldest, Tom is in the middle, and Jane is the youngest.\n",
       "\n",
       "Therefore, Jane is the youngest, as she is younger than both Tom and Sarah.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style='color: #cceeff;'>If Sarah is older than Tom, and Tom is older than Jane, then Jane is the youngest. \n",
       "\n",
       "We can establish the age order as follows:\n",
       "- Sarah > Tom (Sarah is older than Tom)\n",
       "- Tom > Jane (Tom is older than Jane)\n",
       "\n",
       "From this, we can conclude that:\n",
       "- Since Sarah is older than Tom, and Tom is older than Jane, it follows that Sarah is also older than Jane.\n",
       "\n",
       "Thus, Jane is the youngest among the three.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Temperature == 1 maximizes creativity, but also randomness\n",
    "completion = client.chat.completions.create(\n",
    "      model=\"gpt-4o-mini\"\n",
    "    , messages=test_message\n",
    "    , max_tokens=100 # limits the outputted completion_tokens, so will be cut short if it exceeds max_tokens tokens\n",
    "    , temperature=1  # the randomness of the output, 0 is deterministic, 1 is random (repated calls will give different results for 1)\n",
    "    , n=3            # This will generate 3 separate completions\n",
    ")\n",
    "\n",
    "for i, choice in enumerate(completion.choices):\n",
    "    color = PASTELS[i][1]\n",
    "    display(Markdown(f\"<div style='color: {color};'>{choice.message.content}</div>\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The exact same call, but with temperature == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div style='color: #ffcccc;'>If Sarah is older than Tom, and Tom is older than Jane, then Jane is the youngest. \n",
       "\n",
       "We can determine this by analyzing the relationships:\n",
       "- Sarah > Tom (Sarah is older than Tom)\n",
       "- Tom > Jane (Tom is older than Jane)\n",
       "\n",
       "From these two statements, we can infer that:\n",
       "- Sarah > Tom > Jane\n",
       "\n",
       "Since Jane is at the end of this chain, she is the youngest among the three.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style='color: #ccffcc;'>If Sarah is older than Tom, and Tom is older than Jane, then Jane is the youngest. \n",
       "\n",
       "We can determine this by analyzing the relationships:\n",
       "- Sarah > Tom (Sarah is older than Tom)\n",
       "- Tom > Jane (Tom is older than Jane)\n",
       "\n",
       "From these two statements, we can infer that:\n",
       "- Sarah > Tom > Jane\n",
       "\n",
       "Since Jane is at the end of this chain, she is the youngest among the three.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style='color: #cceeff;'>If Sarah is older than Tom, and Tom is older than Jane, then Jane is the youngest. \n",
       "\n",
       "We can determine this by analyzing the relationships:\n",
       "- Sarah > Tom (Sarah is older than Tom)\n",
       "- Tom > Jane (Tom is older than Jane)\n",
       "\n",
       "From these two statements, we can infer that:\n",
       "- Sarah > Tom > Jane\n",
       "\n",
       "Since Jane is at the end of this chain, she is the youngest among the three.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Temperature == 1 maximizes creativity, but also randomness\n",
    "completion = client.chat.completions.create(\n",
    "      model=\"gpt-4o-mini\"\n",
    "    , messages=test_message\n",
    "    , max_tokens=100 # limits the outputted completion_tokens, so will be cut short if it exceeds max_tokens tokens\n",
    "    , temperature=0  # the randomness of the output, 0 is deterministic, 1 is random (repated calls will give different results for 1)\n",
    "    , n=3            # This will generate 3 separate completions\n",
    ")\n",
    "\n",
    "for i, choice in enumerate(completion.choices):\n",
    "    color = PASTELS[i][1]\n",
    "    display(Markdown(f\"<div style='color: {color};'>{choice.message.content}</div>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Load the OpenAI API key from a file into an environment variable called OPENAI_API_KEY\n",
    "%run .load_openai_api_key.py\n",
    "\n",
    "# Instantiate the OpenAI client\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PRICING_PER_M_TOKENS = {\n",
    "    'gpt-4o': {'prompt_tokens': 5.00, 'completion_tokens': 15.00},\n",
    "    'gpt-4o-2024-08-06': {'prompt_tokens': 2.50, 'completion_tokens': 10.00},\n",
    "    'gpt-4o-mini': {'prompt_tokens': 0.150, 'completion_tokens': 0.600},\n",
    "    'gpt-4o-mini-2024-07-18': {'prompt_tokens': 0.150, 'completion_tokens': 0.600},\n",
    "    'o1-preview': {'prompt_tokens': 15.00, 'completion_tokens': 60.00}\n",
    "}\n",
    "\n",
    "def model(persona, prompt, model=\"gpt-4o-mini\"):\n",
    "    completion = client.chat.completions.create(\n",
    "          model=model\n",
    "        , messages=[\n",
    "            { \"role\": \"system\", \"content\": persona},\n",
    "            { \"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "        , temperature=0\n",
    "    )\n",
    "    # Get the pricing for the model used in the completion\n",
    "    pricing = MODEL_PRICING_PER_M_TOKENS[completion.model]\n",
    "\n",
    "    # Calculate the cost of the completion\n",
    "    prompt_cost = completion.usage.prompt_tokens * pricing['prompt_tokens']\n",
    "    generation_cost = completion.usage.completion_tokens * pricing['completion_tokens']\n",
    "    total_cost = (prompt_cost + generation_cost) / 10**6\n",
    "\n",
    "    # Extract the message from the completion\n",
    "    message = completion.choices[0].message.content\n",
    "\n",
    "    return message, total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The capital of France is Paris.', 7.65e-06)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\"you are a helpful assistant\", \"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The capital of France is Paris.', 0.0001275)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\"you are a helpful assistant\", \"What is the capital of France?\", model='gpt-4o-2024-08-06')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The parsing completion object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-ABdLV8se4vnhnwiZ1WEAhGy7zvDpt',\n",
       " 'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='If Sarah is older than Tom, and Tom is older than Jane, then Jane is the youngest. \\n\\nWe can determine this by analyzing the relationships:\\n- Sarah > Tom (Sarah is older than Tom)\\n- Tom > Jane (Tom is older than Jane)\\n\\nFrom these two statements, we can infer that:\\n- Sarah > Tom > Jane\\n\\nSince Jane is at the end of this chain, she is the youngest among the three.', refusal=None, role='assistant', function_call=None, tool_calls=None)),\n",
       "  Choice(finish_reason='stop', index=1, logprobs=None, message=ChatCompletionMessage(content='If Sarah is older than Tom, and Tom is older than Jane, then Jane is the youngest. \\n\\nWe can determine this by analyzing the relationships:\\n- Sarah > Tom (Sarah is older than Tom)\\n- Tom > Jane (Tom is older than Jane)\\n\\nFrom these two statements, we can infer that:\\n- Sarah > Tom > Jane\\n\\nSince Jane is at the end of this chain, she is the youngest among the three.', refusal=None, role='assistant', function_call=None, tool_calls=None)),\n",
       "  Choice(finish_reason='stop', index=2, logprobs=None, message=ChatCompletionMessage(content='If Sarah is older than Tom, and Tom is older than Jane, then Jane is the youngest. \\n\\nWe can determine this by analyzing the relationships:\\n- Sarah > Tom (Sarah is older than Tom)\\n- Tom > Jane (Tom is older than Jane)\\n\\nFrom these two statements, we can infer that:\\n- Sarah > Tom > Jane\\n\\nSince Jane is at the end of this chain, she is the youngest among the three.', refusal=None, role='assistant', function_call=None, tool_calls=None))],\n",
       " 'created': 1727336217,\n",
       " 'model': 'gpt-4o-mini-2024-07-18',\n",
       " 'object': 'chat.completion',\n",
       " 'service_tier': None,\n",
       " 'system_fingerprint': 'fp_1bb46167f9',\n",
       " 'usage': CompletionUsage(completion_tokens=261, prompt_tokens=41, total_tokens=302, completion_tokens_details=CompletionTokensDetails(reasoning_tokens=0))}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_openai_completion_response(completion, print_cost=True):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the pricing for the model used in the completion\n",
    "    pricing = MODEL_PRICING_PER_M_TOKENS[completion.model]\n",
    "\n",
    "    # Calculate the cost of the completion\n",
    "    prompt_cost = completion.usage.prompt_tokens * pricing['prompt_tokens']\n",
    "    generation_cost = completion.usage.completion_tokens * pricing['completion_tokens']\n",
    "    total_cost = (prompt_cost + generation_cost) / 10**6\n",
    "\n",
    "    # Extract the message from the completion\n",
    "    message = completion.choices[0].message.content\n",
    "\n",
    "    # Print the cost of the completion\n",
    "    if print_cost:\n",
    "        print('Cost: cent {:.3f}'.format(total_cost * 100))\n",
    "        print('Cost: USD {:.5f}'.format(total_cost))\n",
    "\n",
    "    return message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: cent 0.016\n",
      "Cost: USD 0.00016\n"
     ]
    }
   ],
   "source": [
    "ans = parse_openai_completion_response(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = parse_openai_completion_response(completion, print_cost=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If Sarah is older than Tom, and Tom is older than Jane, then Jane is the youngest. \n",
      "\n",
      "We can determine this by analyzing the relationships:\n",
      "- Sarah > Tom (Sarah is older than Tom)\n",
      "- Tom > Jane (Tom is older than Jane)\n",
      "\n",
      "From these two statements, we can infer that:\n",
      "- Sarah > Tom > Jane\n",
      "\n",
      "Since Jane is at the end of this chain, she is the youngest among the three.\n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(completion.choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
